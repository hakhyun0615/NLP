# Introduction to Natural Language Processing
This repository contains Jupyter Notebook files summarizing the content of the book "Introduction to natural language processing using deep learning".

## Contents
### 1. Preparing for Natural Language Processing
  1. Numpy
  2. Pandas-Profiling
  3. Machine Learning Workflow
### 2. Text Preprocessing
  1. Tokenization
  2. Cleaning and Normalization
  3. Stemming and Lemmatization
  4. Stopword
  5. Regular Expression
  6. Integer Encoding
  7. Padding
  8. One-Hot Encoding
  9. Splitting Data
  10. Text Preprocessing Tools for Korean Text
### 3. Language Model
  1. What is Language Model?
  2. Statistical Language Model (SLM)
  3. N-gram Language Model
  4. Language Model for Korean Sentences
  5. Perplexity (PPL)
### 4. Count Based Word Representation
  1. Various Ways to Express Words
  2. Bag of Words (BoW)
  3. Document-Term Matrix DTM
  4. Term Frequency-Inverse Document Frequency (TF-IDF)
### 5. Vector Similarity
  1. Cosine Similarity
  2. Various Similarity Techniques
### 6. Machine Learning
  1. What is Machine Learning?
  2. Overview of Machine Learning
  3. Linear Regression
  4. Automatic Differentiation and Linear Regression Practice
  5. Logistic Regression
  6. Logistic Regression Practice
  7. Practice with Multiple Inputs
  8. Vector and Matrix Operations
  9. Softmax Regression
  10. Softmax Regression Practice
### 7. Deep Learning
  1. Perceptron
  2. Overview of Artiicial Neural Network
  3. Understanding Neural Networks through Matrix Multiplication
  4. Learning Methods in Deep Learning
  5. Understanding Backpropagation
  6. Understanding Overfitting Prevention Methods
  7. Gradient Vanishing and Exploding
  8. Overview of Keras
  9. Keras Functional API
  10. Keras Subclassing API
  11. Multi Layer Perceptron for Text Classification
  12. Neural Network Language Model (NNLM)
### 8. Recurrent Nerual Network
  1. Recurrent Nerual Network (RNN)
  2. Long Short-Term Memory (LSTM)
  3. Gated Recurrent Unit (GRU)
  4. Understanding SimpleRNN and LSTM in Keras
  5. Recurrent Neural Network Language Model (RNNLM)
  6. Text Generation using RNN
  7. Char RNN
### 9. Word Embedding
  1. Word Embedding
  2. Word2Vec
  3. Word2Vec Practice in English & Korean
  4. Implementing Word2Vec with Skip-Gram with Negative Sampling (SGNS)
  5. GloVe
  6. FastText
  7. Pre-trained Word Embedding
  8. Embedding Visualization
  9. Recommendation System using Document Embedding
  10. Average Word Embedding
  11. Calculating Similarity of Public Business Reports with Doc2Vec
### 10. Text Classification Using RNN
  1. Spam Detection
  2. Reuters News Classification
  3. IMDB Movie Review Sentiment Analysis
  4. Naive Bayes Classifier
  5. Naver Movie Review Sentiment Analysis
  6. Naver Shopping Review Sentiment Analysis
  7. Sentiment Analysis of Korean Steam Reviews with BiLSTM
### 11. Convolutional Neural Networks for NLP
  1. Convolution Neural Network (CNN)
  2. 1D CNN
  3. IMDB Review Classification with 1D CNN
  4. Spam Email Classification with 1D CNN
  5. Naver Movie Review Classification with Multi-Kernel 1D CNN
  6. Intent Classification using Pre-trained Word Embeddings
  7. Character Embedding
### 12. Tagging Task
  1. Tagging Task using Keras
  2. Part-of-speech Tagging using Bi-LSTM
  3. Named Entity Recognition
  4. BIO Representation in Named Entity Recognition
  5. Named Entity Recognition (NER)
  6. Named Entity Recognition with BiLSTM-CRF
  7. Character Embedding
### 13. Subword Tokenizer
  1. Byte Pair Encoding (BPE)
  2. Sentence Piece
  3. Subword Text Encoder
  4. Huggingface Tokenizer
### 14. Encoder-Decoder Using RNN
  1. Sequence-to-Sequence (Seq2Seq)
  2. Neural Machine Translation using Seq2Seq
  3. BLEU Score (Bilingual Evaluation Understudy Score)
### 15. Attention Mechanism
  1. Attention Mechanism
  2. Bahdanau Attention
  3. BiLSTM with Attention mechanism
### 16. Transformer
  1. Transformer
  2. Transformer Chatbot Tutorial
  3. Multi-head Self Attention for Text Classification
