{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12-07 문자 임베딩(Character Embedding) 활용하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개체명 인식기의 성능을 올리기 위한 방법으로 문자 임베딩을 워드 임베딩과 함께 입력으로 사용하는 방법이 있습니다. 워드 임베딩에 문자 임베딩을 연결(concatenate)하여 성능을 높여봅시다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문자 임베딩(Char Embedding)을 위한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_vocab 만들기\n",
    "words = list(set(data[\"Word\"].values))\n",
    "chars = set([w_i for w in words for w_i in w])\n",
    "chars = sorted(list(chars))\n",
    "print('문자 집합 :',chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index = {c: i + 2 for i, c in enumerate(chars)}\n",
    "char_to_index[\"OOV\"] = 1\n",
    "char_to_index[\"PAD\"] = 0\n",
    "\n",
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_char = 15 # 길이가 15보다 짧은 단어는 뒤에 0으로 패딩\n",
    "\n",
    "# 문자 시퀀스에 대한 패딩하는 함수\n",
    "def padding_char_indice(char_indice, max_len_char):\n",
    "  return pad_sequences(\n",
    "        char_indice, maxlen=max_len_char, padding='post', value = 0)\n",
    "\n",
    "# 각 단어를 문자 시퀀스로 변환 후 패딩 진행\n",
    "def integer_coding(sentences):\n",
    "  char_data = []\n",
    "  for ts in sentences:\n",
    "    word_indice = [word_to_index[t] for t in ts]\n",
    "    char_indice = [[char_to_index[char] for char in t]  \n",
    "                                          for t in ts]\n",
    "    char_indice = padding_char_indice(char_indice, max_len_char)\n",
    "\n",
    "    for chars_of_token in char_indice:\n",
    "      if len(chars_of_token) > max_len_char:\n",
    "        continue\n",
    "    char_data.append(char_indice)\n",
    "  return char_data\n",
    "\n",
    "# 문자 단위 정수 인코딩 결과\n",
    "X_char_data = integer_coding(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩 이전의 기존 문장\n",
    "print('기존 문장 :',sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 단위 정수 인코딩 + 패딩\n",
    "print('단어 단위 정수 인코딩 :')\n",
    "print(X_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 단위 정수 인코딩\n",
    "print('문자 단위 정수 인코딩 :')\n",
    "print(X_char_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char_data = pad_sequences(X_char_data, maxlen=max_len, padding='post', value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char_train, X_char_test, _, _ = train_test_split(X_char_data, y_data, test_size=.2, random_state=777)\n",
    "\n",
    "X_char_train = np.array(X_char_train)\n",
    "X_char_test = np.array(X_char_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_to_word[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join([index_to_char[index] for index in X_char_train[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\n",
    "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
    "print('훈련 샘플 char 데이터의 크기 : {}'.format(X_char_train.shape))\n",
    "print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\n",
    "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM-CNN을 이용한 개체명 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, TimeDistributed, Dropout, concatenate, Bidirectional, LSTM, Conv1D, Dense, MaxPooling1D, Flatten\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "from keras_crf import CRFModel\n",
    "\n",
    "embedding_dim = 128\n",
    "char_embedding_dim = 64\n",
    "dropout_ratio = 0.5\n",
    "hidden_units = 256\n",
    "num_filters = 30\n",
    "kernel_size = 3\n",
    "\n",
    "# 단어 임베딩\n",
    "word_ids = Input(shape=(None,),dtype='int32', name='words_input')\n",
    "word_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(word_ids)\n",
    "\n",
    "# char 임베딩\n",
    "char_ids = Input(shape=(None, max_len_char,), name='char_input')\n",
    "embed_char_out = TimeDistributed(Embedding(len(char_to_index), char_embedding_dim, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(char_ids)\n",
    "dropout = Dropout(dropout_ratio)(embed_char_out)\n",
    "\n",
    "# char 임베딩에 대해서는 Conv1D 수행\n",
    "conv1d_out = TimeDistributed(Conv1D(kernel_size=kernel_size, filters=num_filters, padding='same', activation='tanh', strides=1))(dropout)\n",
    "maxpool_out = TimeDistributed(MaxPooling1D(max_len_char))(conv1d_out)\n",
    "char_embeddings = TimeDistributed(Flatten())(maxpool_out)\n",
    "char_embeddings = Dropout(dropout_ratio)(char_embeddings)\n",
    "\n",
    "# char 임베딩을 Conv1D 수행한 뒤에 단어 임베딩과 연결\n",
    "output = concatenate([word_embeddings, char_embeddings])\n",
    "\n",
    "# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\n",
    "output = Bidirectional(LSTM(hidden_units, return_sequences=True, dropout=dropout_ratio))(output)\n",
    "\n",
    "# 출력층\n",
    "output = TimeDistributed(Dense(tag_size, activation='softmax'))(output)\n",
    "\n",
    "model = Model(inputs=[word_ids, char_ids], outputs=[output])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam',  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('bilstm_cnn.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit([X_train, X_char_train], y_train, batch_size=128, epochs=15, validation_split=0.1, verbose=1, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('bilstm_cnn.h5')\n",
    "\n",
    "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
    "# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
    "y_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])\n",
    "\n",
    "y_predicted = np.argmax(y_predicted, axis=-1) # 확률 벡터를 정수 인코딩으로 변경.\n",
    "labels = np.argmax(y_test[i], -1) # 원-핫 인코딩을 정수 인코딩으로 변경.\n",
    "\n",
    "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
    "print(35 * \"-\")\n",
    "\n",
    "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
    "    if word != 0: # PAD값은 제외함.\n",
    "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict([X_test, X_char_test])\n",
    "pred_tags = sequences_to_tag(y_predicted)\n",
    "test_tags = sequences_to_tag(y_test)\n",
    "\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
    "print(classification_report(test_tags, pred_tags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM-CNN-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "char_embedding_dim = 64\n",
    "dropout_ratio = 0.5\n",
    "hidden_units = 256\n",
    "num_filters = 30\n",
    "kernel_size = 3\n",
    "\n",
    "# 단어 임베딩\n",
    "word_ids = Input(shape=(None,),dtype='int32', name='words_input')\n",
    "word_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(word_ids)\n",
    "\n",
    "# char 임베딩\n",
    "char_ids = Input(shape=(None, max_len_char,), name='char_input')\n",
    "embed_char_out = TimeDistributed(Embedding(len(char_to_index), char_embedding_dim, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(char_ids)\n",
    "dropout = Dropout(dropout_ratio)(embed_char_out)\n",
    "\n",
    "# char 임베딩에 대해서는 Conv1D 수행\n",
    "conv1d_out = TimeDistributed(Conv1D(kernel_size=kernel_size, filters=num_filters, padding='same',activation='tanh', strides=1))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(max_len_char))(conv1d_out)\n",
    "char_embeddings = TimeDistributed(Flatten())(maxpool_out)\n",
    "char_embeddings = Dropout(dropout_ratio)(char_embeddings)\n",
    "\n",
    "# char 임베딩을 Conv1D 수행한 뒤에 단어 임베딩과 연결\n",
    "output = concatenate([word_embeddings, char_embeddings])\n",
    "\n",
    "# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\n",
    "output = Bidirectional(LSTM(hidden_units, return_sequences=True, dropout=dropout_ratio))(output)\n",
    "\n",
    "# 출력층\n",
    "output = TimeDistributed(Dense(tag_size, activation='relu'))(output)\n",
    "\n",
    "base = Model(inputs=[word_ids, char_ids], outputs=[output])\n",
    "model = CRFModel(base, tag_size)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('bilstm_cnn_crf/cp.ckpt', monitor='val_decode_sequence_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X_train, X_char_train], y_train_int, batch_size=128, epochs=15, validation_split=0.1, callbacks=[mc, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('bilstm_cnn_crf/cp.ckpt')\n",
    "\n",
    "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
    "# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
    "y_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])[0] \n",
    "labels = np.argmax(y_test[i], -1) # 원-핫 벡터를 정수 인코딩으로 변경.\n",
    "\n",
    "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
    "print(35 * \"-\")\n",
    "\n",
    "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
    "    if word != 0: # PAD값은 제외함.\n",
    "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict([X_test, X_char_test])[0]\n",
    "pred_tags = sequences_to_tag_for_crf(y_predicted)\n",
    "test_tags = sequences_to_tag(y_test)\n",
    "\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
    "print(classification_report(test_tags, pred_tags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM-BiLSTM-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "char_embedding_dim = 64\n",
    "dropout_ratio = 0.3\n",
    "hidden_units = 64\n",
    "\n",
    "# 단어 임베딩\n",
    "word_ids = Input(batch_shape=(None, None), dtype='int32', name='word_input')\n",
    "word_embeddings = Embedding(input_dim=vocab_size,\n",
    "                                        output_dim=embedding_dim,\n",
    "                                        name='word_embedding')(word_ids)\n",
    "\n",
    "# char 임베딩\n",
    "char_ids = Input(batch_shape=(None, None, None), dtype='int32', name='char_input')\n",
    "char_embeddings = Embedding(input_dim=(len(char_to_index)),\n",
    "                                        output_dim=char_embedding_dim,\n",
    "                                        embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5),\n",
    "                                        name='char_embedding')(char_ids)\n",
    "\n",
    "# char 임베딩을 BiLSTM을 통과 시켜 단어 벡터를 얻고 단어 임베딩과 연결\n",
    "char_embeddings = TimeDistributed(Bidirectional(LSTM(hidden_units)))(char_embeddings)\n",
    "output = concatenate([word_embeddings, char_embeddings])\n",
    "\n",
    "# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\n",
    "output = Dropout(dropout_ratio)(output)\n",
    "output = Bidirectional(LSTM(units=hidden_units, return_sequences=True))(output)\n",
    "\n",
    "# 출력층\n",
    "output = TimeDistributed(Dense(tag_size, activation='relu'))(output)\n",
    "\n",
    "base = Model(inputs=[word_ids, char_ids], outputs=[output])\n",
    "model = CRFModel(base, tag_size)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('bilstm_bilstm_crf/cp.ckpt', monitor='val_decode_sequence_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "history = model.fit([X_train, X_char_train], y_train_int, batch_size=128, epochs=15, validation_split=0.1, callbacks=[mc, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('bilstm_bilstm_crf/cp.ckpt')\n",
    "\n",
    "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
    "# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
    "y_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])[0]\n",
    "labels = np.argmax(y_test[i], -1) # 원-핫 벡터를 정수 인코딩으로 변경.\n",
    "\n",
    "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
    "print(35 * \"-\")\n",
    "\n",
    "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
    "    if word != 0: # PAD값은 제외함.\n",
    "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict([X_test, X_char_test])[0]\n",
    "pred_tags = sequences_to_tag_for_crf(y_predicted)\n",
    "test_tags = sequences_to_tag(y_test)\n",
    "\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
    "print(classification_report(test_tags, pred_tags))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
